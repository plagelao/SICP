---
layout: post
title: Section 1.2.6
date: 2011-07-16 16:00:00 +00:00
categories:
  - sicp
  - scheme
  - lisp
---
Even though this is again a very mathematics related section, it has helped me out to understanding a little bit better the order of growth concept.

- Exercise 1.21.  Use the smallest-divisor procedure to find the smallest divisor of each of the following numbers: 199, 1999, 19999.

  The code for this exercise is [here](https://github.com/plagelao/SICP/blob/master/exercises/chapter-1/exercise-21-definition.scm). The results are:

        >(smallest-divisor 199)
        >199
        >(smallest-divisor 1999)
        >1999
        >(smallest-divisor 19999)
        >7

- Exercise 1.22.  Most Lisp implementations include a primitive called runtime that returns an integer that specifies the amount of time the system has been running (measured, for example, in microseconds). The following timed-prime-test procedure, when called with an integer n, prints n and checks to see if n is prime. If n is prime, the procedure prints three asterisks followed by the amount of time used in performing the test.

        (define (timed-prime-test n)
          (newline)
          (display n)
          (start-prime-test n (runtime)))

        (define (start-prime-test n start-time)
          (if (prime? n)
              (report-prime (- (runtime) start-time))))

        (define (report-prime elapsed-time)
          (display " *** ")
          (display elapsed-time))

  Using this procedure, write a procedure search-for-primes that checks the primality of consecutive odd integers in a specified range. Use your procedure to find the three smallest primes larger than 1000; larger than 10,000; larger than 100,000; larger than 1,000,000. Note the time needed to test each prime. Since the testing algorithm has order of growth of &theta;(&radic;n), you should expect that testing for primes around 10,000 should take about &sqrt;10 times as long as testing for primes around 1000. Do your timing data bear this out? How well do the data for 100,000 and 1,000,000 support the &sqrt;n prediction? Is your result compatible with the notion that programs on your machine run in time proportional to the number of steps required for the computation?

  The procedure to calculate this is:

        (define (timed-prime-test n)
          (define (start-prime-test n start-time)
            (if (prime? n)
                (- (runtime) start-time)
                0))
          (define (prime? n)
            (= n (smallest-divisor n)))
          (start-prime-test n (runtime)))

        (define (search-for-primes minimum maximum time)
          (cond ((= minimum maximum) time)
                (else (search-for-primes (+ minimum 1) maximum (+ time (timed-prime-test minimum))))))

  And the times I get are:

        0.79s for (search-for-primes 10000000000 10000000061 0)
        2.51s for (search-for-primes 100000000000 100000000057 0)
        7.72s for (search-for-primes 1000000000000 1000000000063 0)

  It seems that time grows as expected (from 10000000000 to 1000000000000 time grows 10 times).

- Exercise 1.23.  The smallest-divisor procedure shown at the start of this section does lots of needless testing: After it checks to see if the number is divisible by 2 there is no point in checking to see if it is divisible by any larger even numbers. This suggests that the values used for test-divisor should not be 2, 3, 4, 5, 6, ..., but rather 2, 3, 5, 7, 9, .... To implement this change, define a procedure next that returns 3 if its input is equal to 2 and otherwise returns its input plus 2. Modify the smallest-divisor procedure to use (next test-divisor) instead of (+ test-divisor 1). With timed-prime-test incorporating this modified version of smallest-divisor, run the test for each of the 12 primes found in [exercise 1.22](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_thm_1.22). Since this modification halves the number of test steps, you should expect it to run about twice as fast. Is this expectation confirmed? If not, what is the observed ratio of the speeds of the two algorithms, and how do you explain the fact that it is different from 2?

  My solution for this exercise is:

        (define (fast-smallest-divisor n)
          (define (next n)
            (if (= n 2)
                3
                (+ n 2)))
          (define (find-divisor n test-divisor)
            (cond ((> (square test-divisor) n) n)
                    ((divides? test-divisor n) test-divisor)
                  (else (find-divisor n (next test-divisor)))))
          (define (divides? a b)
            (= (remainder b a) 0))
          (find-divisor n 2))

        (define (fast-timed-prime-test n)
          (define (prime? n)
            (= n (fast-smallest-divisor n)))
          (define (start-prime-test n start-time)
            (if (prime? n)
                (- (runtime) start-time)
                0))
          (start-prime-test n (runtime)))

        (define (fast-search-for-primes minimum maximum time)
          (cond ((= minimum maximum) time)
                (else (fast-search-for-primes (+ minimum 1) maximum (+ time (fast-timed-prime-test minimum))))))

  The times I am getting with this procedure are:

        0.47s for (fast-search-for-primes 10000000000 10000000061 0)
        1.48s for (fast-search-for-primes 100000000000 100000000057 0)
        4.67s for (fast-search-for-primes 1000000000000 1000000000063 0)

  Again, it behaves us we expected.

- Exercise 1.24 Modify the timed-prime-test procedure of [exercise 1.22](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_thm_1.22) to use fast-prime? (the Fermat method), and test each of the 12 primes you found in that exercise. Since the Fermat test has &theta;(log n) growth, how would you expect the time to test primes near 1,000,000 to compare with the time needed to test primes near 1000? Do your data bear this out? Can you explain any discrepancy you find?

  This is the code for this exercise:

        (define (fast-search-for-primes minimum maximum time)
          (define (fast-timed-prime-test n)
            (define (fermat-test n)
                (define (try-it a)
                      (= (expmod a n n) a))
                  (try-it (+ 1 (random (- n 1)))))
            (define (fast-prime? n times)
              (cond ((= times 0) true)
                    ((fermat-test n) (fast-prime? n (- times 1)))
                    (else false)))
            (define (start-prime-test n start-time)
              (if (fast-prime? n 1000)
                (- (runtime) start-time)
                0))
            (start-prime-test n (runtime)))

          (cond ((= minimum maximum) time)
                (else (fast-search-for-primes (+ minimum 1) maximum (+ time (fast-timed-prime-test minimum))))))

  Since this algorithm has &theta;(log n), I expect the calculation for 1,000,000 to take twice the time that the calculation for 1000 takes. The results are as follows:

        0.37s for (fast-search-for-primes 10000000000 10000000061 0)
        0.42s for (fast-search-for-primes 100000000000 100000000057 0)
        0.45s for (fast-search-for-primes 1000000000000 1000000000063 0)

  Which does not confirm it O_o. I do not know what is going on there...

- Exercise 1.25 Alyssa P. Hacker complains that we went to a lot of extra work in writing expmod. After all, she says, since we already know how to compute exponentials, we could have simply written

        (define (expmod base exp m)
          (remainder (fast-expt base exp) m))

  Is she correct? Would this procedure serve as well for our fast prime tester? Explain.

  So, this is the original example:

        (define (expmod base exp m)
          (cond ((= exp 0) 1)
                ((even? exp)
                 (remainder (square (expmod base (/ exp 2) m))
                            m))
                (else
                  (remainder (* base (expmod base (- exp 1) m))
                             m))))

  And here is the fast-expt procedure:

        (define (fast-expt base exp)
          (cond ((= exp 0) 1)
                ((even? exp)
                 (square (fast-expt base (/ exp 2))))
                (else (* base (fast-expt base (- exp 1))))))

  If we use the fast-expt procedure we are going to operate with very large numbers, while the original example always uses "regular" numbers.

- Exercise 1.26 Louis Reasoner is having great difficulty doing [exercise 1.24](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_thm_1.24). His fast-prime? test seems to run more slowly than his prime? test. Louis calls his friend Eva Lu Ator over to help. When they examine Louis's code, they find that he has rewritten the expmod procedure to use an explicit multiplication, rather than calling square:

        (define (expmod base exp m)
          (cond ((= exp 0) 1)
                ((even? exp)
                  (remainder (* (expmod base (/ exp 2) m)
                                (expmod base (/ exp 2) m))
                              m))
                (else
                  (remainder (* base (expmod base (- exp 1) m))
                              m))))

  "I don't see what difference that could make," says Louis. "I do." says Eva. "By writing the procedure like that, you have transformed the &theta;(log n) process into a &theta;(n) process." Explain.

  We are executing expmod twice instead of once, so we need two times the steps of the original algorithm (which was log2).

- Exercise 1.27 Demonstrate that the Carmichael numbers listed in [footnote 47](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#footnote_Temp_80) really do fool the Fermat test. That is, write a procedure that takes an integer n and tests whether a^n is congruent to a modulo n for every a&lt;n, and try your procedure on the given Carmichael numbers.

  This is my Carmichael detector:

        (define (expmod base exp m)
          (cond ((= exp 0) 1)
                ((even? exp)
                  (remainder (square (expmod base (/ exp 2) m))
                  m))
                (else
                  (remainder (* base (expmod base (- exp 1) m))
                  m))))

        (define (carmichael n)
          (define (is-carmichael? n a)
            (if (= a n)
                true
                (if (= (expmod a n n) (expmod a 1 n))
                    (is-carmichael? n (+ a 1))
                    false)))
          (is-carmichael? n 2))

- Exercise 1.28 One variant of the Fermat test that cannot be fooled is called the Miller-Rabin test (Miller 1976; Rabin 1980). This starts from an alternate form of Fermat's Little Theorem, which states that if n is a prime number and a is any positive integer less than n, then a raised to the (n - 1)st power is congruent to 1 modulo n. To test the primality of a number n by the Miller-Rabin test, we pick a random number a&lt;n and raise a to the (n - 1)st power modulo n using the expmod procedure. However, whenever we perform the squaring step in expmod, we check to see if we have discovered a "nontrivial square root of 1 modulo n," that is, a number not equal to 1 or n - 1 whose square is equal to 1 modulo n. It is possible to prove that if such a nontrivial square root of 1 exists, then n is not prime. It is also possible to prove that if n is an odd number that is not prime, then, for at least half the numbers a&lt;n, computing a^(n-1) in this way will reveal a nontrivial square root of 1 modulo n. (This is why the Miller-Rabin test cannot be fooled.) Modify the expmod procedure to signal if it discovers a nontrivial square root of 1, and use this to implement the Miller-Rabin test with a procedure analogous to fermat-test. Check your procedure by testing various known primes and non-primes. Hint: One convenient way to make expmod signal is to have it return 0.

  This is, by large, the most difficult exercise by the moment. It is difficult because it is not easy to understand the mathematics. Here is my solution after some researching:

        (define (miller-rabin-test number)
          (define (exp-modulo base exp)
            (define (square-modulo n)
              (remainder (square n) number))
            (define (square-module-with-check n)
              (if (and (= (square-modulo n) 1)
                  (not (= n 1))
                  (not (= n (- number 1))))
              0
            (square-modulo n)))
          (cond ((= exp 0) 1)
                ((even? exp)
                  (square-module-with-check (exp-modulo base (/ exp 2))))
                (else
                  (remainder (* base (exp-modulo base (- exp 1)))
                             number))))
          (define (try-it a)
            (= (exp-modulo a (- number 1)) 1))
          (try-it (+ 1 (random (- number 1)))))

And that is all for section 1.2. Looking forward to section 1.3
